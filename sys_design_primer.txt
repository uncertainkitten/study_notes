Notes from The System Design Primer: https://github.com/donnemartin/system-design-primer

Lecture on Scalability:
Things to look for in a hosting company:
Due diligence
SFTP versus FTP (Secure, encrypted traffic - particularly for usernames and passwords due to wifi sniffers)
You might want to avoid shared hosting if you're trying to run something that needs to scale
VPS - Virtual Private Server, can rent for yourself - extremely fast/powerful machine chopped up into a bunch of virtual machines using
HyperV or VMWare etc. - Your data is not protected from the webhost itself, of course.
Shared Webhost - 
Only real way to secure data from webhosts is running your own server.
AWS EC2 - allows you to automate scaling by spinning up additional VMs at certain traffic thresholds

Vertical scaling - Make your computer hardware better (add cores, add RAM, add HD space)

Advantage of advances in processors/RAM/HDD size allows splitting hardware more and more efficiently
SATA - usual HD these days - usually 7200 rpm
SAS - Serial Attached SCSI - usually 10K to 15K rpm
Parallel ETA - very old HD
SSDs - no rpm - perform better electronically due to no moving parts

Horizontal scaling: Distributing your systems using cheaper hardware rather than vertically scaling to the state of the art

Challenges: How do you distribute HTTP requests from the client?  Load Balancers (i.e. NGINX)
But how do you determine which server gets the request from the client?

You return the load balancer's IP address, and it determines which application tier machine will get hit.  This also means that you can have private IP
addresses which means you need less IPv4 space

Load balancer can decide which application tier machine handles requests based on how busy a given machine is, distributing work to the machines
with the lightest loads

Another way you could do it is by sharding your databases to handle certain requests, such as videos, images, etc.

But a really simple way to do it is round robin - have the load balancer return an IP address based on which request it is (i.e. first, second, etc.)

Drawbacks of round robin - A user might hit some servers harder than others, and if those requests end up in an odd distribution, some servers might be
slower than others
If you update information on one server, you might get time traveling information - it shows up in one place, but then you re-access and it doesn't show
up on your next server.
Additionally, caching can get you "stuck" to a server, so if you're making heavy requests, the server taking these requests is going to have much
more traffic from you

You can solve some of this by having your load balancer do the round robin:
However, you might have issues with session saving - your session token might be on server one, but server 2 or 3 don't have your session and you end
up on those servers instead at some point and are asked to log in again. Or other information might not be saved on all servers
 - so you might be having problems with user experience of adding a thing to cart, then it goes away, and comes back, etc.

 A way to solve it might be the database sharding - however, you lose redundancy that way

 Another way is having the user get "stuck" to a server temporarily, by the load balancer.  You could also save session data in a machine
 that is connected to all the other machines. Unfortunately, this gives you a single point of failure, again.

 So, another solution is RAID - A thing RAID0 does is striping - it lets you write something to multiple disks, by having the writing follow concurrency
 RAID1 does mirroring - it writes things to both disks simultaneously - this allows you to automatically back up everything
 RAID10 - does striping and redundancy, but takes duplicate disks for all disks
 RAID5 - lets you scale better, only has one redundancy disk
 RAID6 - Has two redundancy disks

 So you could ostensibly use these principles for distributed systems - unfortunately, this still doesn't deal with interruptions to the connections
 between servers

But, the most obvious way to solve this is to have _two_ systems acting as session storage
Software:
Amazon Elastic Load Balancers
HAProxy
LVS
Hardware:
Various companies charge a ton for load balancer machines

Continuing from 46:29
Sticky sessions: Cookies could solve sticky sessions, so that you end up on the same backend server whenever you access a web page that hooks into
a distributed system.  Restrictions: Cookies don't have that much room - counter: You could store the server ID in the cookie.  Issues?  What if the IP
changes?  Also, it seems a bit unnecessary to reveal your IP schema to the world.  Alternatively, you could store a _session token_ or something like that
that the load balancer interprets to route the request correctly

PHP acceleration:
APC (Alternative PHP Cache)
eAccelerator
XCache
Zend platform

You can have the first visit to a site do the full PHP interpretation, but you could afterwards cache PHP authentication so that it's faster in the
future (until you change things)

Caching in general!
.html, MySQL Memory Cache, memcached, etc.

Craigslist: Note that it takes in user input and serves it up as an html file, as opposed to storing the information on a server and serving up the page
dynamically.  This is unusual - it appears that they just flat out store/cache the HTML file they generate and serve it up super fast

There are performance benefits to be had from serving static content - but some of the drawbacks are disk space (though, likely more reads than writes)
There's also a lot of unnecessary redundancy in terms of boilerplate that is just stored over and over. But one of the biggest issues? Changing the
aesthetics of the site is _almost impossible_, because you have thousands and thousands of static pages that would need to be rebuilt to conform
to the new theme.

So, MySQL Query Cache - how does this work?
First time you execute a query, it's slow, but from there, MySQL can cache results and serve those up.

Memcached: Runs on a server, caches whatever you want in RAM.  Like, you can cache queries, or you could cache other things too

Using caches instead of always hitting databases allows you to get much better performance, but caching is fairly limited - it can run out of space



Well, let's continue from 1:04:47 NEXT TIME: https://www.youtube.com/watch?v=-W9F__D3oY4

